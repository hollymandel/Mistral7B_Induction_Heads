{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hollymandel/Mistral7B_Induction_Heads/blob/main/ih_babi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2RP6UB7UjFC"
      },
      "source": [
        "**Impact of ablating induction heads on Mistral 7B performance in a question-answering task**\n",
        "\n",
        "In this notebook I measure the performance of Mistral7B on a single-word question-answering task. After some prompt engineering, Mistral 7B gets approximately 75% of questions right (\"unablated\"). Then I ablate the N most induction-head-like attention heads (see Olsson et al. 2022, https://transformer-circuits.pub), as determined by the scores computed in `ih_sweep.ipynb` and rerun the assessment (\"high score ablations\"). Finally, I instead ablate the N attention heads that are the least induction-head-like, subject to the constraint that the distribution over model layers matches the previously ablated heads (\"low score ablations\").\n",
        "\n",
        "Here are the results for N = 16. I used a largish N to have a more observable effect. However I did not find that ablating induction heads had more of an effect than ablating low-induction-score controls. It would be interesting to dissect the difference between performance on bAbI and performance on the repeated sequence task that was used to measure induction heads. With a higher-compute setup, it would also be interesting to measure the impact of ablating each individual head versus the induction head scores.\n",
        "\n",
        "Unablated: 1481/2000 \\\\\n",
        "High score ablations: 1476/2000 \\\\\n",
        "Low score ablations: 1481/2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318,
          "referenced_widgets": [
            "38f8ba1ef2ba4d0895a2bec27f157d3e",
            "20305b71d0ae475fa9234e2873a1214a",
            "f2a81bc0081946b5ab3a39528cd46f85",
            "8d2c164f53bb4d2bae06da7b1f0a6c76",
            "ffb9521edcb04419a7bc09805eb9d889",
            "a2ddd1e4180c487ab08ab08d100de4d7",
            "c25b32f8787e476e92e2e8328141dbf2",
            "3b6f0e9bdf85420999c57cc94e5fcfd2",
            "c8b8d45b242f41d6904629c12b73299a",
            "251ee19e06da4f7f9563b559b4cdc93b",
            "262b1f4d50ab42edac8cad792805fff6",
            "e0f4a5527dc94a93aabc9233c9cdfa5d",
            "a8ec5e6ea39a46cea3c9667fdfea539a",
            "62efe2a7ed0e45ce9e0bad976e2460d7",
            "e065b20da1164af497a9c28097f30910",
            "d8e7c116bb2e49f2a6a05f6152d914e9",
            "9e8d599afb28409080d6ed97fd0d8f73",
            "e47a5ff748b54ac48966c4da01e43d31",
            "5537e5728a4f49449318d74ed2a0aba1",
            "4b935f5bad3244ad98bdd51397d5c5fc",
            "71db0a64fa0842d883a08c8558fffb39",
            "c2ef105c19d14ad5bf8e1d5fb1ac619e",
            "8baad5501d724bf1a68e76c1b373549c",
            "87a7dcf5f1df45c69d28785d48949d90",
            "70d2caf2d3ac434088b6111644bac1c1",
            "ba3675f913b647948112a6b2147b1ad7",
            "34d3ec16fd474bd4a0f570f53fa5b640",
            "c8669a4d42ab4ee6ba65d0623db3db8d",
            "05869b6758f342f3bbf4434847a77513",
            "6f38eb2655c74936b84a0d3194dd117a",
            "5ee7789a527f446fbaf955120ada6ab8",
            "158f0cdd558848e8a95c3f0c599879ae",
            "e0c41bc980f74be3b3bc32ba9966300e",
            "8406c28fb8374b21bf2388e5709b89d5",
            "30cd28041a99452fb2bd75de5e0e290d",
            "782c4fe2f647468eb3ad105ad425af03",
            "82bfb00090af42a78fa09a42bc80b5d8",
            "6b454bf765b5446092c64ab60af6bffe",
            "e66c4618d4e847d2a93ecc44bee32a5b",
            "08c0198343104251ad285f75c129c2c8",
            "ca15034b68a7456687d3b2b24bc0e091",
            "904061c6feb246b0b5e0f54116d0237e",
            "7280e46ea0eb4ffd8cb292fd4d1b5778",
            "adc7436a944b459093899672c4871151"
          ]
        },
        "id": "AoKGJ7qgSyja",
        "outputId": "5c7485f9-4feb-4c7b-b4b7-ed51bcadcadb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/9.41k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\"\"\" evaluating the model using the bAbI question-answering task from Facebook \"\"\"\n",
        "\n",
        "!pip --q install datasets\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"facebook/babi_qa\",\"en-10k-qa1\",revision=\"main\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaCXB2muSy5v",
        "outputId": "30f012bd-b820-4100-d057-08b5bce9a518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.7/222.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip --q install transformers\n",
        "!pip --q install bitsandbytes accelerate xformers einops # necessary for quantization\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465,
          "referenced_widgets": [
            "46001bbfaf5d4cce803f0afc7cc902e5",
            "16709be1759040e4984d97cd80f64d65",
            "f2ba5b9d1d96460996aee01dcc623749",
            "664e43ffc60b4c95ac87f3af3d40f99b",
            "4aec3ab31e5646dc988b238455da2981",
            "bd79a345ee54433997477a112f4438ad",
            "69bed1edb31e4e688759e47f73dfc3fe",
            "fa8e5bebe78e4a919243d2f7878e165f",
            "7ef50b66cc18466bbb338a4f000c1273",
            "9ab646b50c4b4f6eab7afd35abc6811a",
            "706a67faceb04613af93f7bcdf2926ed",
            "d8f026a00d11461aab5961043f3b50d8",
            "a07c61615f764be09b720a5b0c7cd86d",
            "5b48fd159cf1405c8dd8d7944a957f4d",
            "fb35ebe666024a8c8b91a11082cb4c95",
            "f483a7bc3f204c85b37cdf8040502061",
            "2aab0996ff00491f88b0389489289ea0",
            "b6965ecf59f84e179e27de3bcb446aa8",
            "57dffbdf9df241a585be6ba411f8aa25",
            "c8c4146e00a04314b6a7c4ed992cc8c9",
            "1f6c7d9296cd4140bdc49806cd62f6f7",
            "f28c6fe44d534cabbc72a6c5d1585a33",
            "6c39ff294c8344ac8cc833808359a817",
            "a597fff23ea14f2aa3af9355c80bde61",
            "07aaac95c3354076a6d1f6e3b07f0fba",
            "c5cea5b238594598a4a2c00ef9d5ed36",
            "f16ef57353fc45d0a9db58e0e97128ac",
            "ec89766ba2ba48b8abefbd52268f8553",
            "ad16ecf754824f6ab3cdb727f742d565",
            "513c25686ada443e93d6b10de05ed3f0",
            "00a609da6bfd40a2a1fcc286b6d26dbe",
            "d6c5c64f130745378c5c5a1dbd495fe6",
            "06b83a51bb5c4fa09c78d40a8450a56e",
            "c4b9514ac4bc4098bf1f108ba340bd1a",
            "b73e66fe98024adea344694fb2a8ab41",
            "339186a7055a47848ab214c9dbaba57a",
            "63a661e635d74f80a7695cc64656d626",
            "4a06e324bc4e42838b9d40059e440094",
            "2f96d247015049a299e92a90b7704b37",
            "014c2f8f5ca8479db21e749f677788a4",
            "79c1d0a73d284ffbb8a0ed755ae7e3be",
            "2012b8a3582348edb4e53a579c0d2a46",
            "de3fafd2693c44ac848cfd59ccc39f64",
            "c4378f3cd07942f0bc9cc8d530744847",
            "ed2562ac7b6841dbb8f6973710565878",
            "816050647bad4a09a9ec6db95877393c",
            "960f3141f6a64919bd701819a0f57063",
            "fe296ea8acc1449bb6c4f0400978fca9",
            "dc95d0259b1042fc9d6a1549c788e16c",
            "07476f74d348456e895b5a407acc0582",
            "b948dbe8238a4075b39e9daed9c440f2",
            "b22bb96581654aa6b5e3afde046b8d6c",
            "9c7b009e0bdc4eb7a70f40f755702823",
            "c539a454118548f5922349b7a1f37c7c",
            "126b300ed4124f1ea301186846bc8138",
            "be8083edf09c4e16adca158ef17b7897",
            "4af79ec3f6244a4180f8dbe0547c65b6",
            "3561a572a8194aecba71e14fe527e146",
            "b28274bf511b478590f09aca17ed98d2",
            "8dd4ddd176f64ee08443677bf41ea113",
            "46bba01a00834a479daa3f97e6c4af99",
            "46e61a032089495199357d270384dbe2",
            "42b8087808c94d18b4e23be7997997dc",
            "dd7b9e930ee14c06ac703ec6363ad0c8",
            "24a04edb4c864761a89b91f4bbad4e08",
            "b6a8fc91c417461f92affff084da90a6",
            "635c350e9cb24b2b8e8c0530af4e464b",
            "7583e53741dd44409c1f77334c7be832",
            "0ea1a1b537a0444d8b979640961eed4f",
            "6ac2ce6be74e4e01890095847abd0d52",
            "260a2150c1a4430dbb7fafb8a72d8788",
            "dee81153ff3a4dd38d75e55be6671eb4",
            "909ab80ddbe44bf9901cf49f6c047024",
            "f78a2bb35523489bb0b96e1abd92cab8",
            "911f9bd42c294f84b4d33fb3723fb726",
            "5186c2a19fdf4d6f90bd5d767d993fe8",
            "a5ea05a0fe7147c9b83e5300f83fa832",
            "2a4f755dec1e4495900fc3dc6187ef50",
            "6a94e7f7c705425ebdbecd9beb259017",
            "11bf3079c1fc44b78e10b93add70c429",
            "6c8013ddd5c546e797424a6c3823c06c",
            "fc4eccc71ee8444e9c8cb618958cf5f5",
            "1c2587edd3184aca8a8b005927e5eb99",
            "3001549caf12427e9e0148e1b7877942",
            "2e2b33b661e84412bd9fb25996430e76",
            "21233ecda1b04c778352d2cfec75c092",
            "41103b5ac4744f15a31898ce060c6b8d",
            "2d571e2049664dc987c507bd829db004",
            "951cdae0236c477fa1d983d887b3d46c",
            "8fd4b08c28344514a46aaa9480e683b2",
            "6e160241acb54e01a0d5a50fdd466820",
            "1cc977acb80a4090be0073a0906213e9",
            "5993373732e1478ebaa25485472240bf",
            "a15f512b59dc4621aae8deeeaa971c23",
            "fba8371d24ff4fbc99abd8df05961844",
            "3a4794c9e62c436d9c821df2634c7de8",
            "a524a549ca834e1da8ab17e5553499d5",
            "f2031c100bf544c2b4e66f20fcea743b",
            "6d1d486891af475da58c5642497572c2",
            "58188f30c95047b19f645017beffb8a6",
            "e50d4e75185a403b9b88a3621339a887",
            "8ddb93101f7043b9bee12c84daf82cb4",
            "dff994d0bd054465a318d43f1ec469af",
            "a0b6b13d973a4ff58d50b799804eee9e",
            "a39af809e5c94e62b6ace6631162a063",
            "414ed63e28604d2c9e478e8f50b686e7",
            "c5ab67bfeca2444784e2e5a5ce86e790",
            "cb95ccecdc9b4bc8b43a213778d83da5",
            "7a55fffcfe224ef3950ac562c551725d",
            "d5709dd26f3d4dab968bc443b49db48d",
            "970b8902bdf74744b7fa67fdc09836aa",
            "bca38fd1f6784b10a23d434d5af2d1a4",
            "cb1b6f314a784391beb84ef3dd141f30",
            "a1b9963a09f84ffa8b1f77cf35c265ee",
            "caac2b02884542ccaa03ed9912487174",
            "8afa81581ec141e0a27d7ebce060bac4",
            "3633a49d49014d27995753df7b0a6344",
            "e94a4cb12515404c9fb3a2af720132d2",
            "ea67c8d03eff43839e770b303bc20a67",
            "c509eb7ff4b04edbb90b97b3cbaa3da5",
            "96858c45d67c4bc386f672a2fd818669",
            "65cc0f9977934301a5e001c27a0f6d7e",
            "7366416eff7a4a87a30f1afb6a0eeef5",
            "3bd2e12b62894f969aeedd4f13d25d7b",
            "4373b0d73bce45fa9d5431bc60ef30a4",
            "97cee5e45b314c139daf64fde4db07de",
            "430a6bab97304dcdae16052b3a41bd17",
            "1eaa1bb611bc42838c9839400d049c50",
            "03f05788ba904b369d79be97c007ecc5",
            "772141c47a804b13ab1f781083b6a5b8",
            "b68732af76ef415194cbc31591e7f645",
            "e1c7cb3815824c4285ce03bee8269bb4",
            "6b601c2d377c49348a8f33cdeb4f8426",
            "614207302e634ed6a6292f5a23510d3a",
            "fc277786b6874afd8dfbcdb3dab09c74",
            "2286519e0a594f3ca9918f7b062de623",
            "1bb299f87e5047dc9ea7aa9b8d6f785e",
            "f3efab21e7c74429aa78805b41c3dae0",
            "855988cae0b44666a51398521938c1ba",
            "48167853c30e4d8f82a0d0a4cfa55e07",
            "6436823424a54f6596e67aeedb12f27c",
            "0842a4326aa04b1ea4ab66786a12cf8b",
            "16b81620cfe34fdb8c34ce38dc6fe038",
            "532592ca298445358aa9ca1d1535d401",
            "86894d9d1b0d472a9420adb5d51fabf7",
            "06695be4819d4f509675ca0f71a13722",
            "6f1caa6dbb7f4ebc9e2b2f0304e560d4",
            "c3a5d48663d6489aa98d4f9bf28b746c",
            "6f21d989d63c4f08b529ce8c714a15dd",
            "76e35a63c01e43a682198ed84640d08a",
            "de88bc10af834843a04db5d59063d33f",
            "3150577a96194982bce1c1bd74dcaec9",
            "488c77f580954014a9ae1710f3032fd6",
            "3562c12fe928437e844285c4d762482e"
          ]
        },
        "id": "eioICUPwTyzj",
        "outputId": "30ca602a-8095-436d-93d5-a9b1a79e57a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "pytorch_model-00001-of-00006.bin:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "pytorch_model-00002-of-00006.bin:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "pytorch_model-00003-of-00006.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "pytorch_model-00004-of-00006.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "pytorch_model-00005-of-00006.bin:   0%|          | 0.00/4.83G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "pytorch_model-00006-of-00006.bin:   0%|          | 0.00/4.25G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/918 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\"\"\" using a quantized and sharded version of Mistral 7B. Thanks to Hugo Fernandez for\n",
        "engineering the model to run on a single T4. Note that this block takes 5-10 minutes to run.\"\"\"\n",
        "\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "\n",
        "model_id = \"Hugofernandez/Mistral-7B-v0.1-colab-sharded\"\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config = bnb_config,\n",
        "    device_map = \"auto\",\n",
        "    attn_implementation=\"eager\")\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
        "start_token = tokenizer.bos_token\n",
        "end_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1J3sXt8yTzMA"
      },
      "outputs": [],
      "source": [
        "N_HEADS_ABLATE = 16\n",
        "\n",
        "with open(\"ih_scores_dict.pkl\", \"rb\") as f:\n",
        "  ih_scores_dict = pickle.load(f)\n",
        "\n",
        "def top_values(scores_dict, n):\n",
        "  \"\"\" extract the top n scorers from scores_dict. Scores_dict is formatted as\n",
        "  key: vector of scores. Output formatted as (score, key, index in vector). \"\"\"\n",
        "  flattened = []\n",
        "  for key, vector in scores_dict.items():\n",
        "      for idx, value in enumerate(vector):\n",
        "          flattened.append((value, key, idx))\n",
        "\n",
        "  sorted_flattened = sorted(flattened, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "  return sorted_flattened[:n]\n",
        "\n",
        "top_ih = top_values(ih_scores_dict, n = N_HEADS_ABLATE) # List[(score, layer, head index)]\n",
        "\n",
        "# count the layers in top_ih\n",
        "top_ih_layers = [ layer for _, layer, _ in top_ih ]\n",
        "top_ih_layers = { key: sum(x == key for x in top_ih_layers) for key in top_ih_layers }\n",
        "\n",
        "# pick lowest-scoring heads subject to matching layer multiplicity. Also formatted as\n",
        "# List[(score, layer, head index)]\n",
        "matched_list = []\n",
        "for layer, count in top_ih_layers.items():\n",
        "  vector = ih_scores_dict[layer].copy()\n",
        "  labelled = [ (value, layer, idx) for idx, value in enumerate(vector) ]\n",
        "  sorted_labelled = sorted(labelled, key=lambda x: x[0], reverse=False)\n",
        "  matched_list.extend(sorted_labelled[:count])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "PGgoo6UAYFhg",
        "outputId": "b05103fa-83c5-4aec-ea8b-fc0245e0da85"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Read the following passage and answer the questions with a single word.\\n\\nPassage: Mary moved to the bathroom.\\nJohn went to the hallway.\\n\\nQuestion: Where is Mary?\\nAnswer: bathroom\\nPassage: Daniel went back to the hallway.\\nSandra moved to the garden.\\n\\nQuestion: Where is Daniel?\\nAnswer: hallway\\nPassage: John moved to the office.\\nSandra journeyed to the bathroom.\\n\\nQuestion: Where is Daniel?\\nAnswer: hallway\\nPassage: Mary moved to the hallway.\\nDaniel travelled to the office.\\n\\nQuestion: Where is Daniel?\\nAnswer: office\\nPassage: John went back to the garden.\\nJohn moved to the bedroom.\\n\\nQuestion: Where is Sandra?\\nAnswer: '"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "PROMPT_HEADER = \"Read the following passage and answer the questions with a single word.\\n\\n\"\n",
        "\n",
        "def create_prompt(story, header = PROMPT_HEADER, start = 0, end = np.infty):\n",
        "  \"\"\" Mistral 7B performs poorly on the \"raw task\", so this function appends an instructional\n",
        "  header and then gives several examples of successful answering before asking the question.\n",
        "  But note that context from earlier questions is relevant to later questions.\"\"\"\n",
        "  BLOCK_LENGTH = 3\n",
        "\n",
        "  prompt = header\n",
        "  for i, (text, answer) in enumerate(zip(story[\"text\"], story[\"answer\"])):\n",
        "    top = min(len(story[\"text\"]),end)\n",
        "    if i < start or i >= end:\n",
        "      continue\n",
        "    # task is a list formatted as [ passage, question, answer, ... ]\n",
        "    if (i+1) % BLOCK_LENGTH != 0:\n",
        "      if (i+1) % BLOCK_LENGTH == 1:\n",
        "        prompt += \"Passage: \"\n",
        "      prompt += f\"{text}\\n\"\n",
        "    else:\n",
        "      prompt += \"\\nQuestion: \" + f\"{text}\\n\"\n",
        "      prompt += \"Answer: \"\n",
        "      if i != top-1:\n",
        "        prompt += f\"{answer}\\n\"\n",
        "\n",
        "  return prompt\n",
        "\n",
        "\"\"\" Example prompt \"\"\"\n",
        "create_prompt(dataset[\"train\"][\"story\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3m-ZAeNtuHhR"
      },
      "outputs": [],
      "source": [
        "MODEL_SIZE = 128 # internal representation dimension of Mistral 7B\n",
        "\n",
        "def score_model(model, story, margin = 3, verbose = 0, start = 0, end = np.infty):\n",
        "  \"\"\" Format the prompt using the prompt header and several examples, get `margin` tokens of output,\n",
        "  and then mark as correct if the solution is contained in the output -- a bit liberal, to allow for\n",
        "  some formatting tokens, etc. \"\"\"\n",
        "\n",
        "  if end == np.infty:\n",
        "    end = len(story[\"id\"]) # the end parameter is not robust to off-block choices\n",
        "\n",
        "  prompt = create_prompt(story, start = start, end = end)\n",
        "  prompt_encode = tokenizer.encode(prompt, return_tensors = \"pt\", padding=True, truncation=True)\n",
        "\n",
        "  prompt_len = len(prompt_encode[0])\n",
        "  output = model.generate(prompt_encode, max_length=prompt_len + margin, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "  decode = tokenizer.decode(output[0][prompt_len:], padding=True, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "  parsed = re.findall(r'\\w+', decode)\n",
        "\n",
        "  if verbose >= 2:\n",
        "    print(prompt)\n",
        "  if verbose >= 1:\n",
        "    print(parsed)\n",
        "    print(f\"desired: {story['answer'][end-1]}\")\n",
        "\n",
        "  if story[\"answer\"][end-1] in parsed:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def insert_ablation_hook(head_index):\n",
        "  def hook(module, input, output):\n",
        "      output[:,:,head_index*MODEL_SIZE:(head_index+1)*MODEL_SIZE].data.zero_()\n",
        "  return hook\n",
        "\n",
        "def ablation_experiment(model, ablation_list, print_pd = 5):\n",
        "  \"\"\" Ablate the heads in ablation_list, compute performance on score_model for the entire training\n",
        "  dataset, make sure to remove hooks no matter what happenes. Ablation list is assumed formatted as\n",
        "  (score, head, layer), though score is not used here. \"\"\"\n",
        "\n",
        "  try:\n",
        "    ablation_hooks = {}\n",
        "    for i, (_, layer, head) in enumerate(ablation_list):\n",
        "      ablation_hooks[i] = model.model.layers[layer].post_attention_layernorm.register_forward_hook(insert_ablation_hook(head))\n",
        "\n",
        "    win = 0\n",
        "    lose = 0\n",
        "    lose_list = []\n",
        "    for i, story in enumerate(dataset[\"train\"][\"story\"]):\n",
        "      if score_model(model, story,verbose=0,end=np.infty):\n",
        "        win += 1\n",
        "      else:\n",
        "        lose += 1\n",
        "        lose_list.append(i)\n",
        "\n",
        "      if i % print_pd == 0:\n",
        "        print(f\"score: {win}/{win+lose}\")\n",
        "\n",
        "  finally:\n",
        "    try:\n",
        "      for hook in ablation_hooks.values():\n",
        "        hook.remove()\n",
        "      print(\"successfully removed all hooks\")\n",
        "    except Exception as e:\n",
        "      raise Exception(\"Critical Error: failure to remove ablation hook\") from e\n",
        "\n",
        "  return win, lose, lose_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDqewj0ygzSN"
      },
      "outputs": [],
      "source": [
        "\"\"\" run ablation experiment \"\"\"\n",
        "\n",
        "win_hs_abl, lose_hs_abl, lose_hs_list = ablation_experiment(model, top_ih) # 16: 1489/2000\n",
        "# win_ls_abl, lose_ls_abl, lose_ls_list = ablation_experiment(model, matched_list) # 16: 1476/2000\n",
        "# win_unabl, lose_unabl, lose_unable_list = ablation_experiment(model, []) # 16: 1481/2000"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM5gV2OHKiFAOy6PQp+PCH0",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}